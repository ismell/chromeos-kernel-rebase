From b07234b56415d9b76ba71c9794395b58ba6d27fa Mon Sep 17 00:00:00 2001
From: Yu Zhao <yuzhao@google.com>
Date: Mon, 26 Nov 2018 14:22:10 -0700
Subject: [PATCH] CHROMIUM: mm: collect swap and reclaim metrics

Collect swap refault distances, swap read latencies and direct
reclaim latencies. The feature is controlled by CONFIG_MM_METRICS,
and is not configured by default. The metrics are provided to
userspace via /sys/kernel/debug/mm_metrics/{swap_refault,
swap_latency,reclaim_latency}. These files are histograms and they
are disabled by default. To enable them, write whitespace-separated
strictly-increasing list of upper bounds to the files (see examples
below). The lower bound of the first bucket is always zero, and the
upper bound of the last bucket is always U64_MAX, which can be
omitted when writing the files. Writing the upper bounds again frees
old buckets and creates new ones. Writing "clear" clears existing
counts. Writing "disable" stops counting and frees histograms. The
max number of buckets is 100.

We squeeze swapout timestamp into PTE. This eliminates the need for
extra memory but limits the largest timestamp value we can store. On
arm64 and x86_86 without L1TF bug, we use 25 bits for the timestamp
and the rest for swap offset. Swap refault distance is measured in
seconds so the timestamp won't exceed 2^25 seconds with one year of
uptime. When it does, we stop timestamping pages at swapout, and
those pages won't be counted at swappin. The max supported size of
swapfile is reduced from 2^50*4KB to 2^25*4KB (128G). On 32-bit CPU
or x86_64 with L1TF bug, there isn't enough space in PTE for the
timestamp. /sys/kernel/debug/mm_metrics/swap_refault isn't available
in this case.

Swap read latency and direct reclaim latency are measured in
nanosecond.

Here are some examples:
  # cat /sys/kernel/debug/mm_metrics/reclaim_latency
  disabled
  # seq 100 100 1000 >/sys/kernel/debug/mm_metrics/reclaim_latency
  # cat /sys/kernel/debug/mm_metrics/reclaim_latency
  0-100 0
  101-200 0
  201-300 0
  301-400 0
  401-500 0
  501-600 0
  601-700 0
  701-800 0
  801-900 0
  901-1000 0
  1001-18446744073709551615 43324
  # echo clear >/sys/kernel/debug/mm_metrics/reclaim_latency
  0-100 0
  101-200 0
  201-300 0
  301-400 0
  401-500 0
  501-600 0
  601-700 0
  701-800 0
  801-900 0
  901-1000 0
  1001-18446744073709551615 0
  # echo disable >/sys/kernel/debug/mm_metrics/reclaim_latency
  # cat /sys/kernel/debug/mm_metrics/reclaim_latency
  disabled

BUG=b:119687058
TEST=Enabled the metrics and checked the debugfs files

Change-Id: I93f94c01258b229e3ca84232ab43eb2d79580eb5
Signed-off-by: Yu Zhao <yuzhao@google.com>
Reviewed-on: https://chromium-review.googlesource.com/1351583
Commit-Ready: ChromeOS CL Exonerator Bot <chromiumos-cl-exonerator@appspot.gserviceaccount.com>
Tested-by: Yu Zhao <yuzhao@chromium.org>
Reviewed-by: Yu Zhao <yuzhao@chromium.org>
---
 include/linux/mm_metrics.h |  84 ++++++++++
 include/linux/swapops.h    |  61 ++++++-
 mm/Kconfig                 |   9 ++
 mm/Makefile                |   1 +
 mm/memory.c                |   7 +-
 mm/metrics.c               | 314 +++++++++++++++++++++++++++++++++++++
 mm/page_alloc.c            |   6 +
 mm/shmem.c                 |  22 ++-
 mm/swap_state.c            |   2 +
 mm/swapfile.c              |  11 +-
 10 files changed, 501 insertions(+), 16 deletions(-)
 create mode 100644 include/linux/mm_metrics.h
 create mode 100644 mm/metrics.c

diff --git a/include/linux/mm_metrics.h b/include/linux/mm_metrics.h
new file mode 100644
index 000000000000..c699517aa40e
--- /dev/null
+++ b/include/linux/mm_metrics.h
@@ -0,0 +1,84 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+
+#ifndef _LINUX_MM_METRICS_H
+#define _LINUX_MM_METRICS_H
+
+#include <linux/timekeeping.h>
+#include <linux/sched/clock.h>
+#include <linux/swapops.h>
+
+struct histogram {
+	struct rcu_head rcu;
+	unsigned int size;
+	u64 __percpu *buckets;
+	u64 thresholds[0];
+};
+
+enum {
+	MM_SWAP_REFAULT,
+	MM_SWAP_LATENCY,
+	MM_RECLAIM_LATENCY,
+	NR_MM_METRICS,
+};
+
+extern struct histogram __rcu *mm_metrics_files[NR_MM_METRICS];
+
+#ifdef CONFIG_MM_METRICS
+#define mm_metrics_enabled(type)	rcu_access_pointer(mm_metrics_files[type])
+#else
+#define mm_metrics_enabled(type)	0
+#endif
+
+extern void mm_metrics_record(unsigned int type, u64 val, u64 count);
+
+static inline void mm_metrics_swapout(swp_entry_t *swap)
+{
+	if (mm_metrics_enabled(MM_SWAP_REFAULT)) {
+		u64 start = ktime_get_seconds();
+
+		VM_BUG_ON(swp_type(*swap) >= MAX_SWAPFILES);
+		VM_BUG_ON(!swp_offset(*swap));
+
+		swap->val &= ~GENMASK_ULL(SWP_TM_OFF_BITS - 1, SWP_OFFSET_BITS);
+		if (start < BIT_ULL(SWP_TIME_BITS))
+			swap->val |= start << SWP_OFFSET_BITS;
+	}
+}
+
+static inline void mm_metrics_swapin(swp_entry_t swap)
+{
+	if (mm_metrics_enabled(MM_SWAP_REFAULT)) {
+		u64 start = _swp_offset(swap) >> SWP_OFFSET_BITS;
+
+		VM_BUG_ON(swp_type(swap) >= MAX_SWAPFILES);
+		VM_BUG_ON(!swp_offset(swap));
+
+		if (start)
+			mm_metrics_record(MM_SWAP_REFAULT,
+					  ktime_get_seconds() - start, 1);
+	}
+}
+
+static inline u64 mm_metrics_swapin_start(void)
+{
+	return mm_metrics_enabled(MM_SWAP_LATENCY) ? sched_clock() : 0;
+}
+
+static inline void mm_metrics_swapin_end(u64 start)
+{
+	if (mm_metrics_enabled(MM_SWAP_LATENCY) && start)
+		mm_metrics_record(MM_SWAP_LATENCY, sched_clock() - start, 1);
+}
+
+static inline u64 mm_metrics_reclaim_start(void)
+{
+	return mm_metrics_enabled(MM_RECLAIM_LATENCY) ? sched_clock() : 0;
+}
+
+static inline void mm_metrics_reclaim_end(u64 start)
+{
+	if (mm_metrics_enabled(MM_RECLAIM_LATENCY) && start)
+		mm_metrics_record(MM_RECLAIM_LATENCY, sched_clock() - start, 1);
+}
+
+#endif /* _LINUX_MM_METRICS_H */
diff --git a/include/linux/swapops.h b/include/linux/swapops.h
index 22af9d8a84ae..074bb896b4c1 100644
--- a/include/linux/swapops.h
+++ b/include/linux/swapops.h
@@ -47,7 +47,7 @@ static inline unsigned swp_type(swp_entry_t entry)
  * Extract the `offset' field from a swp_entry_t.  The swp_entry_t is in
  * arch-independent format
  */
-static inline pgoff_t swp_offset(swp_entry_t entry)
+static inline pgoff_t _swp_offset(swp_entry_t entry)
 {
 	return entry.val & SWP_OFFSET_MASK(entry);
 }
@@ -82,7 +82,7 @@ static inline pte_t swp_entry_to_pte(swp_entry_t entry)
 {
 	swp_entry_t arch_entry;
 
-	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));
+	arch_entry = __swp_entry(swp_type(entry), _swp_offset(entry));
 	return __swp_entry_to_pte(arch_entry);
 }
 
@@ -102,6 +102,63 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)
 	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);
 }
 
+/*
+ * We squeeze swapout timestamp into swp_offset because we don't
+ * want to allocate extra memory for it. Normally we have 50 bits
+ * in swp_offset on x86_64 and arm64. So we use 25 bits for the
+ * timestamp and the rest for offset. The timestamp is uptime in
+ * second, and it won't overflow within one year. The max size of
+ * swapfile is 128G, which is more than enough for now. If we have
+ * less than 50 bits in swp_offset due to 32-bit swp_entry_t or
+ * X86_BUG_L1TF, we don't enable the timestamp.
+ */
+#define SWP_TIME_BITS	25
+#define SWP_OFFSET_BITS	25
+#define SWP_TM_OFF_BITS	(SWP_TIME_BITS + SWP_OFFSET_BITS)
+
+extern bool swap_refault_enabled __read_mostly;
+
+#ifdef CONFIG_MM_METRICS
+
+static inline pgoff_t swp_offset(swp_entry_t swap)
+{
+	return swap_refault_enabled && swp_type(swap) < MAX_SWAPFILES ?
+	       _swp_offset(swap) & GENMASK_ULL(SWP_OFFSET_BITS - 1, 0) :
+	       _swp_offset(swap);
+}
+
+static inline bool swp_entry_same(swp_entry_t s1, swp_entry_t s2)
+{
+	return swp_type(s1) == swp_type(s2) && swp_offset(s1) == swp_offset(s2);
+}
+
+static inline bool swp_page_same(swp_entry_t swap, struct page *page)
+{
+	swp_entry_t entry = { .val = page_private(page) };
+
+	VM_BUG_ON(!PageSwapCache(page));
+
+	return swp_entry_same(swap, entry);
+}
+
+static inline bool swp_radix_same(swp_entry_t swap, void *radix)
+{
+	return radix_tree_exceptional_entry(radix) &&
+	       swp_entry_same(swap, radix_to_swp_entry(radix));
+}
+
+#else /* CONFIG_MM_METRICS */
+
+#define swp_offset(swap)		_swp_offset(swap)
+
+#define swp_entry_same(s1, s2)		((s1).val == (s2).val)
+
+#define swp_page_same(swap, page)	((swap).val == page_private(page))
+
+#define swp_radix_same(swap, radix)	(swp_to_radix_entry(swap) == (radix))
+
+#endif /* CONFIG_MM_METRICS */
+
 #if IS_ENABLED(CONFIG_DEVICE_PRIVATE)
 static inline swp_entry_t make_device_private_entry(struct page *page, bool write)
 {
diff --git a/mm/Kconfig b/mm/Kconfig
index b79c03ac7060..c132591704a2 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -792,4 +792,13 @@ config GUP_BENCHMARK
 config ARCH_HAS_PTE_SPECIAL
 	bool
 
+config MM_METRICS
+	bool "Collect additional memory statistics"
+	help
+	  Collect swap refault distances (seconds), swap read latencies and direct
+	  reclaim latencies (nanoseconds). They are provided userspace via
+	  /sys/kernel/debug/mm_metrics/{swap_refault,swap_latency,reclaim_latency}
+	  in histograms.
+	default n
+
 endmenu
diff --git a/mm/Makefile b/mm/Makefile
index a99fda5f12cd..1370d6626857 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -106,3 +106,4 @@ obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
 obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
 obj-$(CONFIG_HMM) += hmm.o
 obj-$(CONFIG_MEMFD_CREATE) += memfd.o
+obj-$(CONFIG_MM_METRICS) += metrics.o
diff --git a/mm/memory.c b/mm/memory.c
index c467102a5cbc..6dfb8c6f008d 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -69,6 +69,7 @@
 #include <linux/userfaultfd_k.h>
 #include <linux/dax.h>
 #include <linux/oom.h>
+#include <linux/mm_metrics.h>
 
 #include <asm/io.h>
 #include <asm/mmu_context.h>
@@ -2907,6 +2908,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	int locked;
 	int exclusive = 0;
 	vm_fault_t ret = 0;
+	u64 start = 0;
 
 	if (!pte_unmap_same(vma->vm_mm, vmf->pmd, vmf->pte, vmf->orig_pte))
 		goto out;
@@ -2935,12 +2937,14 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 
 
 	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
+	mm_metrics_swapin(entry);
 	page = lookup_swap_cache(entry, vma, vmf->address);
 	swapcache = page;
 
 	if (!page) {
 		struct swap_info_struct *si = swp_swap_info(entry);
 
+		start = mm_metrics_swapin_start();
 		if (si->flags & SWP_SYNCHRONOUS_IO &&
 				__swap_count(si, entry) == 1) {
 			/* skip swapcache */
@@ -2988,6 +2992,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 
 	locked = lock_page_or_retry(page, vma->vm_mm, vmf->flags);
 
+	mm_metrics_swapin_end(start);
 	delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
 	if (!locked) {
 		ret |= VM_FAULT_RETRY;
@@ -3001,7 +3006,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	 * swapcache, we need to check that the page's swap has not changed.
 	 */
 	if (unlikely((!PageSwapCache(page) ||
-			page_private(page) != entry.val)) && swapcache)
+			!swp_page_same(entry, page))) && swapcache)
 		goto out_page;
 
 	page = ksm_might_need_to_copy(page, vma, vmf->address);
diff --git a/mm/metrics.c b/mm/metrics.c
new file mode 100644
index 000000000000..81f3d9f41f43
--- /dev/null
+++ b/mm/metrics.c
@@ -0,0 +1,314 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+#include <linux/ctype.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/swap.h>
+#include <linux/swapfile.h>
+#include <linux/debugfs.h>
+#include <linux/mm_metrics.h>
+
+/* make sure swapout timestamp won't wrap around within a year */
+#define SECONDS_PER_YEAR	(60 * 60 * 24 * 365)
+/* max number of buckets for histogram */
+#define MAX_HISTOGRAM_SIZE	100
+/* max number of digits in decimal for threshold plus one space */
+#define MAX_CHARS_PER_THRESHOLD	(20 + 1)
+
+bool swap_refault_enabled __read_mostly;
+struct histogram __rcu *mm_metrics_files[NR_MM_METRICS];
+
+static const char *const mm_metrics_names[] = {
+	"swap_refault",
+	"swap_latency",
+	"reclaim_latency",
+};
+
+static DEFINE_SPINLOCK(histogram_lock);
+
+static struct histogram *histogram_alloc(const u64 *thresholds,
+					 unsigned int size)
+{
+	int i;
+	int len;
+	struct histogram *hist;
+
+	VM_BUG_ON(!size || size > MAX_HISTOGRAM_SIZE);
+
+	len = sizeof(struct histogram) + size * sizeof(*hist->thresholds);
+	hist = kmalloc(len, GFP_ATOMIC);
+	if (!hist)
+		return ERR_PTR(-ENOMEM);
+
+	len = size * sizeof(*hist->buckets);
+	hist->buckets = __alloc_percpu_gfp(len, __alignof__(*hist->buckets),
+					   GFP_ATOMIC);
+	if (!hist->buckets) {
+		kfree(hist);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	hist->size = size;
+	for (i = 0; i < size; i++) {
+		VM_BUG_ON(i && thresholds[i - 1] >= thresholds[i]);
+
+		hist->thresholds[i] = thresholds[i];
+	}
+	VM_BUG_ON(thresholds[i - 1] != U64_MAX);
+
+	return hist;
+}
+
+static struct histogram *histogram_create(char *buf)
+{
+	int i;
+	unsigned int size;
+	u64 *thresholds;
+	struct histogram *hist;
+
+	if (!*buf)
+		return ERR_PTR(-EINVAL);
+
+	thresholds = kmalloc_array(MAX_HISTOGRAM_SIZE, sizeof(*thresholds),
+				   GFP_KERNEL);
+	if (!thresholds)
+		return ERR_PTR(-ENOMEM);
+
+	for (i = 0; i < MAX_HISTOGRAM_SIZE; i++) {
+		thresholds[i] = simple_strtoull(buf, &buf, 0);
+		if (!*buf)
+			break;
+
+		if (!isspace(*buf)) {
+			hist = ERR_PTR(-EINVAL);
+			goto failed;
+		}
+
+		while (isspace(*buf))
+			buf++;
+	}
+
+	if (i == MAX_HISTOGRAM_SIZE) {
+		hist = ERR_PTR(-E2BIG);
+		goto failed;
+	}
+
+	/* the last theshold must be U64_MAX, add it if missing */
+	if (thresholds[i++] != U64_MAX) {
+		if (i == MAX_HISTOGRAM_SIZE) {
+			hist = ERR_PTR(-E2BIG);
+			goto failed;
+		}
+		thresholds[i++] = U64_MAX;
+	}
+
+	size = i;
+
+	for (i = 1; i < size; i++) {
+		if (thresholds[i - 1] >= thresholds[i]) {
+			hist = ERR_PTR(-EINVAL);
+			goto failed;
+		}
+	}
+
+	hist = histogram_alloc(thresholds, size);
+failed:
+	kfree(thresholds);
+
+	return hist;
+}
+
+static void histogram_free(struct rcu_head *rcu)
+{
+	struct histogram *hist = container_of(rcu, struct histogram, rcu);
+
+	VM_BUG_ON(!hist->size || hist->size > MAX_HISTOGRAM_SIZE);
+
+	free_percpu(hist->buckets);
+	kfree(hist);
+}
+
+static int mm_metrics_read(struct seq_file *sf, void *v)
+{
+	int i;
+	int cpu;
+	u64 *buckets;
+	struct histogram *hist;
+	int rc = 0;
+	unsigned int type = (unsigned long)sf->private;
+
+	VM_BUG_ON(type >= NR_MM_METRICS);
+
+	rcu_read_lock();
+
+	hist = rcu_dereference(mm_metrics_files[type]);
+	if (!hist) {
+		seq_puts(sf, "disabled\n");
+		goto unlock;
+	}
+
+	VM_BUG_ON(!hist->size || hist->size > MAX_HISTOGRAM_SIZE);
+
+	buckets = kmalloc_array(hist->size, sizeof(*buckets), GFP_NOWAIT);
+	if (!buckets) {
+		rc = -ENOMEM;
+		goto unlock;
+	}
+
+	memset(buckets, 0, hist->size * sizeof(*buckets));
+
+	for_each_possible_cpu(cpu) {
+		for (i = 0; i < hist->size; i++)
+			buckets[i] += per_cpu(hist->buckets[i], cpu);
+	}
+
+	for (i = 0; i < hist->size; i++) {
+		u64 lower = i ? hist->thresholds[i - 1] + 1 : 0;
+		u64 upper = hist->thresholds[i];
+
+		VM_BUG_ON(lower > upper);
+
+		seq_printf(sf, "%llu-%llu %llu\n", lower, upper, buckets[i]);
+	}
+	VM_BUG_ON(hist->thresholds[i - 1] != U64_MAX);
+
+	kfree(buckets);
+unlock:
+	rcu_read_unlock();
+
+	return rc;
+}
+
+static ssize_t mm_metrics_write(struct file *file, const char __user *buf,
+				size_t len, loff_t *ppos)
+{
+	char *raw, *trimmed;
+	struct histogram *old, *new = NULL;
+	unsigned int type = (unsigned long)file_inode(file)->i_private;
+
+	VM_BUG_ON(type >= NR_MM_METRICS);
+
+	if (len > MAX_HISTOGRAM_SIZE * MAX_CHARS_PER_THRESHOLD)
+		return -E2BIG;
+
+	raw = memdup_user_nul(buf, len);
+	if (IS_ERR(raw))
+		return PTR_ERR(raw);
+
+	trimmed = strim(raw);
+	if (!strcmp(trimmed, "clear")) {
+		rcu_read_lock();
+		old = rcu_dereference(mm_metrics_files[type]);
+		if (old)
+			new = histogram_alloc(old->thresholds, old->size);
+		rcu_read_unlock();
+	} else if (strcmp(trimmed, "disable"))
+		new = histogram_create(trimmed);
+
+	kfree(raw);
+
+	if (IS_ERR(new))
+		return PTR_ERR(new);
+
+	spin_lock(&histogram_lock);
+	old = rcu_dereference_protected(mm_metrics_files[type],
+					lockdep_is_held(&histogram_lock));
+	rcu_assign_pointer(mm_metrics_files[type], new);
+	spin_unlock(&histogram_lock);
+	if (old)
+		call_rcu(&old->rcu, histogram_free);
+
+	return len;
+}
+
+static int mm_metrics_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mm_metrics_read, inode->i_private);
+}
+
+static const struct file_operations mm_metrics_ops = {
+	.open		= mm_metrics_open,
+	.write		= mm_metrics_write,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int __init mm_metrics_init(void)
+{
+	int i;
+	struct dentry *dent;
+#ifdef CONFIG_SWAP
+	unsigned long now = ktime_get_seconds();
+	unsigned long size = max_swapfile_size();
+
+	if (SWP_TM_OFF_BITS > FIELD_SIZEOF(swp_entry_t, val) * BITS_PER_BYTE)
+		pr_err("swap refault metrics disabled: 32-bit CPU\n");
+	else if (size < GENMASK_ULL(SWP_TM_OFF_BITS - 1, 0) + 1)
+		pr_err("swap refault metrics disabled: size %ld\n", size);
+	else if (now + SECONDS_PER_YEAR > BIT_ULL(SWP_TIME_BITS))
+		pr_err("swap refault metrics disabled: time %ld\n", now);
+	else
+		swap_refault_enabled = true;
+#endif
+
+	BUILD_BUG_ON(ARRAY_SIZE(mm_metrics_names) != NR_MM_METRICS);
+
+	if (!debugfs_initialized())
+		return -ENODEV;
+
+	dent = debugfs_create_dir("mm_metrics", NULL);
+	if (!dent)
+		return -ENODEV;
+
+	for (i = 0; i < NR_MM_METRICS; i++) {
+		struct dentry *fent;
+
+		if (i == MM_SWAP_REFAULT && !swap_refault_enabled)
+			continue;
+
+		fent = debugfs_create_file(mm_metrics_names[i], 0644, dent,
+					   (void *)(long)i, &mm_metrics_ops);
+		if (IS_ERR_OR_NULL(fent)) {
+			debugfs_remove_recursive(dent);
+
+			return -ENODEV;
+		}
+	}
+
+	pr_info("memory metrics initialized\n");
+
+	return 0;
+}
+subsys_initcall(mm_metrics_init);
+
+void mm_metrics_record(unsigned int type, u64 val, u64 count)
+{
+	int lower, upper;
+	struct histogram *hist;
+
+	VM_BUG_ON(type >= NR_MM_METRICS);
+
+	rcu_read_lock();
+
+	hist = rcu_dereference(mm_metrics_files[type]);
+	if (!hist)
+		goto unlock;
+
+	VM_BUG_ON(!hist->size || hist->size > MAX_HISTOGRAM_SIZE);
+
+	lower = 0;
+	upper = hist->size - 1;
+	while (lower < upper) {
+		int i = (lower + upper) >> 1;
+
+		if (val <= hist->thresholds[i])
+			upper = i;
+		else
+			lower = i + 1;
+	}
+
+	this_cpu_add(hist->buckets[upper], count);
+unlock:
+	rcu_read_unlock();
+}
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 60cc6ebe6a7a..dbca32a5e0fe 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -67,6 +67,7 @@
 #include <linux/lockdep.h>
 #include <linux/nmi.h>
 #include <linux/low-mem-notify.h>
+#include <linux/mm_metrics.h>
 
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
@@ -4371,6 +4372,7 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	unsigned int alloc_flags = ALLOC_WMARK_LOW;
 	gfp_t alloc_mask; /* The gfp_t that was actually used for allocation */
 	struct alloc_context ac = { };
+	u64 start = 0;
 
 	gfp_mask &= gfp_allowed_mask;
 	alloc_mask = gfp_mask;
@@ -4405,7 +4407,11 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	if (unlikely(ac.nodemask != nodemask))
 		ac.nodemask = nodemask;
 
+	if (order < MAX_ORDER && (gfp_mask & __GFP_DIRECT_RECLAIM) &&
+	    !(current->flags & PF_MEMALLOC))
+		start = mm_metrics_reclaim_start();
 	page = __alloc_pages_slowpath(alloc_mask, order, &ac);
+	mm_metrics_reclaim_end(start);
 
 out:
 	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
diff --git a/mm/shmem.c b/mm/shmem.c
index 0c3b005a59eb..18b18516caa0 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -74,6 +74,7 @@ static struct vfsmount *shm_mnt;
 #include <linux/magic.h>
 #include <linux/syscalls.h>
 #include <linux/fcntl.h>
+#include <linux/mm_metrics.h>
 #include <uapi/linux/memfd.h>
 #include <linux/userfaultfd_k.h>
 #include <linux/rmap.h>
@@ -358,7 +359,7 @@ static bool shmem_confirm_swap(struct address_space *mapping,
 	rcu_read_lock();
 	item = radix_tree_lookup(&mapping->i_pages, index);
 	rcu_read_unlock();
-	return item == swp_to_radix_entry(swap);
+	return swp_radix_same(swap, item);
 }
 
 /*
@@ -1110,7 +1111,8 @@ static void shmem_evict_inode(struct inode *inode)
 	clear_inode(inode);
 }
 
-static unsigned long find_swap_entry(struct radix_tree_root *root, void *item)
+static unsigned long find_swap_entry(struct radix_tree_root *root,
+				     swp_entry_t swap, void **item)
 {
 	struct radix_tree_iter iter;
 	void __rcu **slot;
@@ -1125,7 +1127,8 @@ static unsigned long find_swap_entry(struct radix_tree_root *root, void *item)
 			slot = radix_tree_iter_retry(&iter);
 			continue;
 		}
-		if (entry == item) {
+		if (swp_radix_same(swap, entry)) {
+			*item = entry;
 			found = iter.index;
 			break;
 		}
@@ -1152,8 +1155,7 @@ static int shmem_unuse_inode(struct shmem_inode_info *info,
 	gfp_t gfp;
 	int error = 0;
 
-	radswap = swp_to_radix_entry(swap);
-	index = find_swap_entry(&mapping->i_pages, radswap);
+	index = find_swap_entry(&mapping->i_pages, swap, &radswap);
 	if (index == -1)
 		return -EAGAIN;	/* tell shmem_unuse we found nothing */
 
@@ -1232,7 +1234,7 @@ int shmem_unuse(swp_entry_t swap, struct page *page)
 	 * There's a faint possibility that swap page was replaced before
 	 * caller locked it: caller will come back later with the right page.
 	 */
-	if (unlikely(!PageSwapCache(page) || page_private(page) != swap.val))
+	if (unlikely(!PageSwapCache(page) || !swp_page_same(swap, page)))
 		goto out;
 
 	/*
@@ -1362,6 +1364,7 @@ static int shmem_writepage(struct page *page, struct writeback_control *wbc)
 		spin_unlock_irq(&info->lock);
 
 		swap_shmem_alloc(swap);
+		mm_metrics_swapout(&swap);
 		shmem_delete_from_page_cache(page, swp_to_radix_entry(swap));
 
 		mutex_unlock(&shmem_swaplist_mutex);
@@ -1678,9 +1681,13 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 	charge_mm = vma ? vma->vm_mm : current->mm;
 
 	if (swap.val) {
+		u64 start = 0;
+
+		mm_metrics_swapin(swap);
 		/* Look it up and read it in.. */
 		page = lookup_swap_cache(swap, NULL, 0);
 		if (!page) {
+			start = mm_metrics_swapin_start();
 			/* Or update major stats only when swapin succeeds?? */
 			if (fault_type) {
 				*fault_type |= VM_FAULT_MAJOR;
@@ -1697,7 +1704,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 
 		/* We have to do this with page locked to prevent races */
 		lock_page(page);
-		if (!PageSwapCache(page) || page_private(page) != swap.val ||
+		if (!PageSwapCache(page) || !swp_page_same(swap, page) ||
 		    !shmem_confirm_swap(mapping, index, swap)) {
 			error = -EEXIST;	/* try again */
 			goto unlock;
@@ -1706,6 +1713,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 			error = -EIO;
 			goto failed;
 		}
+		mm_metrics_swapin_end(start);
 		wait_on_page_writeback(page);
 
 		if (shmem_should_replace_page(page, gfp)) {
diff --git a/mm/swap_state.c b/mm/swap_state.c
index ecee9c6c4cc1..940bbcb48551 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -21,6 +21,7 @@
 #include <linux/vmalloc.h>
 #include <linux/swap_slots.h>
 #include <linux/huge_mm.h>
+#include <linux/mm_metrics.h>
 
 #include <asm/pgtable.h>
 
@@ -227,6 +228,7 @@ int add_to_swap(struct page *page)
 	/*
 	 * Add it to the swap cache.
 	 */
+	mm_metrics_swapout(&entry);
 	err = add_to_swap_cache(page, entry,
 			__GFP_HIGH|__GFP_NOMEMALLOC|__GFP_NOWARN);
 	/* -ENOMEM radix-tree allocation failure */
diff --git a/mm/swapfile.c b/mm/swapfile.c
index 1a4b3363f44f..1b18853070c8 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -1743,9 +1743,9 @@ unsigned int count_swap_pages(int type, int free)
 }
 #endif /* CONFIG_HIBERNATION */
 
-static inline int pte_same_as_swp(pte_t pte, pte_t swp_pte)
+static inline int pte_same_as_swp(pte_t pte, swp_entry_t swp)
 {
-	return pte_same(pte_swp_clear_soft_dirty(pte), swp_pte);
+	return is_swap_pte(pte) && swp_entry_same(pte_to_swp_entry(pte), swp);
 }
 
 /*
@@ -1774,7 +1774,7 @@ static int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,
 	}
 
 	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
-	if (unlikely(!pte_same_as_swp(*pte, swp_entry_to_pte(entry)))) {
+	if (unlikely(!pte_same_as_swp(*pte, entry))) {
 		mem_cgroup_cancel_charge(page, memcg, false);
 		ret = 0;
 		goto out;
@@ -1813,7 +1813,6 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				unsigned long addr, unsigned long end,
 				swp_entry_t entry, struct page *page)
 {
-	pte_t swp_pte = swp_entry_to_pte(entry);
 	pte_t *pte;
 	int ret = 0;
 
@@ -1832,7 +1831,7 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 		 * swapoff spends a _lot_ of time in this loop!
 		 * Test inline before going to call unuse_pte.
 		 */
-		if (unlikely(pte_same_as_swp(*pte, swp_pte))) {
+		if (unlikely(pte_same_as_swp(*pte, entry))) {
 			pte_unmap(pte);
 			ret = unuse_pte(vma, pmd, addr, entry, page);
 			if (ret)
@@ -2207,7 +2206,7 @@ int try_to_unuse(unsigned int type, bool frontswap,
 		 * delete, since it may not have been written out to swap yet.
 		 */
 		if (PageSwapCache(page) &&
-		    likely(page_private(page) == entry.val) &&
+		    likely(swp_page_same(entry, page)) &&
 		    !page_swapped(page))
 			delete_from_swap_cache(compound_head(page));
 
-- 
2.23.0.rc1.153.gdeed80330f-goog

